{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UApWofI0M5gH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('/content/tictac.csv')\n",
        "\n",
        "# Preprocessing: Encode 'x', 'o', and 'b' to numeric values\n",
        "data.replace({'x': 1, 'o': -1, 'b': 0}, inplace=True)\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data.drop(columns=['class'])\n",
        "y = data['class']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and testing set (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "TWGLdwT0NS1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a292vCNqTdjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y):\n",
        "    if len(y) == 0:\n",
        "        return 0\n",
        "    counts = Counter(y)\n",
        "    probabilities = [count / len(y) for count in counts.values()]\n",
        "    return -sum(p * math.log2(p) for p in probabilities if p > 0)\n",
        "\n",
        "def information_gain(y, y_left, y_right):\n",
        "    p = len(y_left) / len(y)\n",
        "    return entropy(y) - p * entropy(y_left) - (1 - p) * entropy(y_right)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion='entropy', max_depth=None):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        if len(set(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        best_gain = 0\n",
        "        best_split = None\n",
        "        for column in X.columns:\n",
        "            values = X[column].unique()\n",
        "            for value in values:\n",
        "                left_mask = X[column] == value\n",
        "                right_mask = ~left_mask\n",
        "                y_left, y_right = y[left_mask], y[right_mask]\n",
        "                gain = information_gain(y, y_left, y_right) if self.criterion == 'entropy' else gini_index(y, y_left, y_right)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_split = (column, value)\n",
        "\n",
        "        if best_split is None:\n",
        "            return Counter(y).most_common(1)[0][0]\n",
        "\n",
        "        column, value = best_split\n",
        "        left_mask = X[column] == value\n",
        "        right_mask = ~left_mask\n",
        "        left_tree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
        "        right_tree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "        return (column, value, left_tree, right_tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_one(row) for _, row in X.iterrows()]\n",
        "\n",
        "    def _predict_one(self, row):\n",
        "        node = self.tree\n",
        "        while isinstance(node, tuple):\n",
        "            column, value, left_tree, right_tree = node\n",
        "            node = left_tree if row[column] == value else right_tree\n",
        "        return node\n",
        "\n",
        "# Training the decision tree model\n",
        "dt_entropy = DecisionTree(criterion='entropy')\n",
        "dt_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_entropy = dt_entropy.predict(X_test)\n",
        "print(\"Entropy-based Decision Tree Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_entropy))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_entropy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NoPk6OhNePL",
        "outputId": "49496057-a726-4526-b47c-40b142d2c97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy-based Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.93      0.93      0.93        67\n",
            "        True       0.96      0.96      0.96       125\n",
            "\n",
            "    accuracy                           0.95       192\n",
            "   macro avg       0.94      0.94      0.94       192\n",
            "weighted avg       0.95      0.95      0.95       192\n",
            "\n",
            "Accuracy: 0.9479166666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_index(y, y_left, y_right):\n",
        "    def gini(y):\n",
        "        m = len(y)\n",
        "        return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y))\n",
        "\n",
        "    m = len(y)\n",
        "    m_left, m_right = len(y_left), len(y_right)\n",
        "    return (m_left / m) * gini(y_left) + (m_right / m) * gini(y_right)\n"
      ],
      "metadata": {
        "id": "CcjaXJoxTi2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(y, y_left, y_right):\n",
        "    def entropy(y):\n",
        "        m = len(y)\n",
        "        return -sum((np.sum(y == c) / m) * np.log2(np.sum(y == c) / m) for c in np.unique(y))\n",
        "\n",
        "    m = len(y)\n",
        "    m_left, m_right = len(y_left), len(y_right)\n",
        "    return entropy(y) - (m_left / m) * entropy(y_left) - (m_right / m) * entropy(y_right)\n"
      ],
      "metadata": {
        "id": "W3xH7E3vTqlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, criterion='gini', max_depth=None):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "        if n_labels == 1 or n_samples <= 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        best_gain = -1\n",
        "        split_idx, split_thresh = None, None\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            X_column = X[:, feature_idx]\n",
        "            thresholds = np.unique(X_column)\n",
        "            for threshold in thresholds:\n",
        "                left_mask = X_column <= threshold\n",
        "                right_mask = ~left_mask\n",
        "                y_left, y_right = y[left_mask], y[right_mask]\n",
        "                gain = information_gain(y, y_left, y_right) if self.criterion == 'entropy' else gini_index(y, y_left, y_right)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    split_idx = feature_idx\n",
        "                    split_thresh = threshold\n",
        "\n",
        "        if best_gain == -1:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        left_mask = X[:, split_idx] <= split_thresh\n",
        "        right_mask = ~left_mask\n",
        "        left_subtree = self._build_tree(X[left_mask, :], y[left_mask], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_mask, :], y[right_mask], depth + 1)\n",
        "        return Node(split_idx, split_thresh, left_subtree, right_subtree)\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        return np.bincount(y).argmax()\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n"
      ],
      "metadata": {
        "id": "DQ-c0mdRTy86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {\n",
        "    'A': [1, 2, 3],\n",
        "    'B': [4, 5, 6],\n",
        "    'C': [7, 8, 9]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Correct indexing using .iloc for integer-based indexing\n",
        "try:\n",
        "    print(df.iloc[:, 0])  # Access all rows in the first column\n",
        "except Exception as e:\n",
        "    print(f\"Error with iloc: {e}\")\n",
        "\n",
        "# Correct indexing using .loc for label-based indexing\n",
        "try:\n",
        "    print(df.loc[:, 'A'])  # Access all rows in column 'A'\n",
        "except Exception as e:\n",
        "    print(f\"Error with loc: {e}\")\n",
        "\n",
        "# Slicing example\n",
        "try:\n",
        "    print(df.iloc[:, :2])  # Access all rows and the first two columns\n",
        "except Exception as e:\n",
        "    print(f\"Error with slicing: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ES6MX2TUa9r",
        "outputId": "c546a108-5c8a-4e8b-fc84-58c9ffea6f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    1\n",
            "1    2\n",
            "2    3\n",
            "Name: A, dtype: int64\n",
            "0    1\n",
            "1    2\n",
            "2    3\n",
            "Name: A, dtype: int64\n",
            "   A  B\n",
            "0  1  4\n",
            "1  2  5\n",
            "2  3  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Training scikit-learn decision tree classifier with entropy-based criterion\n",
        "sklearn_dt_entropy = DecisionTreeClassifier(criterion='entropy')\n",
        "sklearn_dt_entropy.fit(X_train, y_train)\n",
        "y_pred_sklearn_entropy = sklearn_dt_entropy.predict(X_test)\n",
        "print(\"scikit-learn Entropy-based Decision Tree Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_sklearn_entropy))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_sklearn_entropy))\n",
        "\n",
        "# Training scikit-learn decision tree classifier with Gini index\n",
        "sklearn_dt_gini = DecisionTreeClassifier(criterion='gini')\n",
        "sklearn_dt_gini.fit(X_train, y_train)\n",
        "y_pred_sklearn_gini = sklearn_dt_gini.predict(X_test)\n",
        "print(\"scikit-learn Gini Index-based Decision Tree Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_sklearn_gini))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_sklearn_gini))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGVguI0iUdIR",
        "outputId": "4ccfe095-443f-4708-bb00-698350865110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn Entropy-based Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.98      0.96      0.97        67\n",
            "        True       0.98      0.99      0.98       125\n",
            "\n",
            "    accuracy                           0.98       192\n",
            "   macro avg       0.98      0.97      0.98       192\n",
            "weighted avg       0.98      0.98      0.98       192\n",
            "\n",
            "Accuracy: 0.9791666666666666\n",
            "scikit-learn Gini Index-based Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.97      0.96      0.96        67\n",
            "        True       0.98      0.98      0.98       125\n",
            "\n",
            "    accuracy                           0.97       192\n",
            "   macro avg       0.97      0.97      0.97       192\n",
            "weighted avg       0.97      0.97      0.97       192\n",
            "\n",
            "Accuracy: 0.9739583333333334\n"
          ]
        }
      ]
    }
  ]
}